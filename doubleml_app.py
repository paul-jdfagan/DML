{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6bcdcd-a992-458e-a7e7-503bde86f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from flaml import AutoML\n",
    "import doubleml as dml\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(page_title=\"DoubleML Causal Analysis\", layout=\"wide\", page_icon=\"üìä\")\n",
    "\n",
    "# Custom RMSE function\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Generic temporal transformer\n",
    "class TemporalFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Generic temporal feature engineering for time series data\"\"\"\n",
    "    \n",
    "    def __init__(self, date_col=\"Date\", include_cyclical=True, include_calendar=True, \n",
    "                 include_trend=True, include_retail_events=False):\n",
    "        self.date_col = date_col\n",
    "        self.include_cyclical = include_cyclical\n",
    "        self.include_calendar = include_calendar\n",
    "        self.include_trend = include_trend\n",
    "        self.include_retail_events = include_retail_events\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df[self.date_col] = pd.to_datetime(df[self.date_col])\n",
    "        df = df.sort_values(self.date_col).reset_index(drop=True)\n",
    "        \n",
    "        features_added = []\n",
    "\n",
    "        # Cyclical encodings\n",
    "        if self.include_cyclical:\n",
    "            df[\"month_sin\"] = np.sin(2*np.pi * df[self.date_col].dt.month/12)\n",
    "            df[\"month_cos\"] = np.cos(2*np.pi * df[self.date_col].dt.month/12)\n",
    "            df[\"dow_sin\"] = np.sin(2*np.pi * df[self.date_col].dt.weekday/7)\n",
    "            df[\"dow_cos\"] = np.cos(2*np.pi * df[self.date_col].dt.weekday/7)\n",
    "            df[\"doy_sin\"] = np.sin(2*np.pi * df[self.date_col].dt.dayofyear/365)\n",
    "            df[\"doy_cos\"] = np.cos(2*np.pi * df[self.date_col].dt.dayofyear/365)\n",
    "            features_added.extend([\"month_sin\", \"month_cos\", \"dow_sin\", \"dow_cos\", \"doy_sin\", \"doy_cos\"])\n",
    "\n",
    "        # Calendar flags\n",
    "        if self.include_calendar:\n",
    "            df[\"is_weekend\"] = (df[self.date_col].dt.weekday >= 5).astype(int)\n",
    "            df[\"is_month_start\"] = (df[self.date_col].dt.day <= 7).astype(int)\n",
    "            df[\"is_month_end\"] = (df[self.date_col].dt.day >= 24).astype(int)\n",
    "            df[\"is_quarter_start\"] = df[self.date_col].dt.month.isin([1,4,7,10]).astype(int)\n",
    "            df[\"is_quarter_end\"] = df[self.date_col].dt.month.isin([3,6,9,12]).astype(int)\n",
    "            features_added.extend([\"is_weekend\", \"is_month_start\", \"is_month_end\", \n",
    "                                 \"is_quarter_start\", \"is_quarter_end\"])\n",
    "\n",
    "        # Retail/shopping season flags (optional)\n",
    "        if self.include_retail_events:\n",
    "            df[\"is_holiday_season\"] = (\n",
    "                (df[self.date_col].dt.month==12) |\n",
    "                ((df[self.date_col].dt.month==11)&(df[self.date_col].dt.day>=15))\n",
    "            ).astype(int)\n",
    "            df[\"is_black_friday_week\"] = (\n",
    "                (df[self.date_col].dt.month==11)&(df[self.date_col].dt.day>=20)\n",
    "            ).astype(int)\n",
    "            df[\"is_nordstrom_anniversary_sale\"] = (\n",
    "                ((df[self.date_col].dt.month==7)&(df[self.date_col].dt.day>=15)) |\n",
    "                ((df[self.date_col].dt.month==8)&(df[self.date_col].dt.day<=10))\n",
    "            ).astype(int)\n",
    "            df[\"is_sephora_spring_sale\"] = (\n",
    "                (df[self.date_col].dt.month==4)&\n",
    "                df[self.date_col].dt.day.between(1,20)\n",
    "            ).astype(int)\n",
    "            df[\"is_walmart_july_event\"] = (\n",
    "                (df[self.date_col].dt.month==7)&\n",
    "                df[self.date_col].dt.day.between(10,20)\n",
    "            ).astype(int)\n",
    "            df[\"is_target_july_event\"] = df[\"is_walmart_july_event\"]\n",
    "            df[\"is_amazon_prime_days_window\"] = (\n",
    "                (df[self.date_col].dt.month==7)&\n",
    "                df[self.date_col].dt.day.between(9,18)\n",
    "            ).astype(int)\n",
    "            features_added.extend([\n",
    "                \"is_holiday_season\", \"is_black_friday_week\", \"is_nordstrom_anniversary_sale\",\n",
    "                \"is_sephora_spring_sale\", \"is_walmart_july_event\", \"is_target_july_event\",\n",
    "                \"is_amazon_prime_days_window\"\n",
    "            ])\n",
    "\n",
    "        # Trend features\n",
    "        if self.include_trend:\n",
    "            df[\"time_trend\"] = np.arange(len(df))\n",
    "            df[\"time_trend_sq\"] = df[\"time_trend\"] ** 2\n",
    "            features_added.extend([\"time_trend\", \"time_trend_sq\"])\n",
    "\n",
    "        return df.drop(columns=[self.date_col]), features_added\n",
    "\n",
    "def geometric_adstock(s, alpha=0.9, lags=7):\n",
    "    \"\"\"Apply geometric adstock transformation\"\"\"\n",
    "    w = alpha ** np.arange(lags)\n",
    "    return s.rolling(lags, min_periods=1).apply(lambda x: np.dot(x[::-1], w[:len(x)]), raw=True)\n",
    "\n",
    "def apply_lag_features(df, col, lags=[1, 7, 14, 30]):\n",
    "    \"\"\"Apply simple lag features\"\"\"\n",
    "    for lag in lags:\n",
    "        df[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "    return df\n",
    "\n",
    "# Initialize session state\n",
    "if 'df' not in st.session_state:\n",
    "    st.session_state.df = None\n",
    "if 'results' not in st.session_state:\n",
    "    st.session_state.results = None\n",
    "if 'dml_plr' not in st.session_state:\n",
    "    st.session_state.dml_plr = None\n",
    "\n",
    "# Title\n",
    "st.title(\"üìä Generic DoubleML Causal Analysis Tool\")\n",
    "st.markdown(\"*Flexible time series causal inference for any domain*\")\n",
    "st.markdown(\"---\")\n",
    "\n",
    "# Sidebar\n",
    "with st.sidebar:\n",
    "    st.header(\"Navigation\")\n",
    "    step = st.radio(\n",
    "        \"Select Step:\",\n",
    "        [\"1Ô∏è‚É£ Upload Data\", \"2Ô∏è‚É£ Configure Analysis\", \"3Ô∏è‚É£ Run Analysis\", \"4Ô∏è‚É£ Sensitivity Analysis\"],\n",
    "        key=\"nav_step\"\n",
    "    )\n",
    "    st.markdown(\"---\")\n",
    "    st.info(\"üí° **Tip**: Follow the steps in order for best results.\")\n",
    "    \n",
    "    with st.expander(\"‚ÑπÔ∏è About This Tool\"):\n",
    "        st.markdown(\"\"\"\n",
    "        This tool implements **Double Machine Learning (DoubleML)** for causal inference on time series data.\n",
    "        \n",
    "        **Use cases:**\n",
    "        - Marketing: Ad spend ‚Üí Sales\n",
    "        - Healthcare: Treatment ‚Üí Outcomes\n",
    "        - Policy: Intervention ‚Üí Impact\n",
    "        - Finance: Events ‚Üí Returns\n",
    "        - Operations: Changes ‚Üí Metrics\n",
    "        \"\"\")\n",
    "\n",
    "# Step 1: Upload Data\n",
    "if step == \"1Ô∏è‚É£ Upload Data\":\n",
    "    st.header(\"Step 1: Upload Your Dataset\")\n",
    "    \n",
    "    col1, col2 = st.columns([2, 1])\n",
    "    \n",
    "    with col1:\n",
    "        uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
    "    \n",
    "    with col2:\n",
    "        st.info(\"**Requirements:**\\n- Time series data\\n- Date column\\n- Outcome variable\\n- Treatment variable\")\n",
    "    \n",
    "    if uploaded_file is not None:\n",
    "        df = pd.read_csv(uploaded_file)\n",
    "        st.session_state.df = df\n",
    "        \n",
    "        st.success(f\"‚úÖ Dataset uploaded successfully! {len(df)} rows √ó {len(df.columns)} columns\")\n",
    "        \n",
    "        tab1, tab2, tab3 = st.tabs([\"üìä Preview\", \"üìà Summary Stats\", \"üîç Data Quality\"])\n",
    "        \n",
    "        with tab1:\n",
    "            st.dataframe(df.head(20), use_container_width=True)\n",
    "        \n",
    "        with tab2:\n",
    "            col1, col2 = st.columns(2)\n",
    "            with col1:\n",
    "                st.write(\"**Numeric Columns:**\")\n",
    "                st.dataframe(df.describe())\n",
    "            with col2:\n",
    "                st.write(\"**Data Types:**\")\n",
    "                st.dataframe(pd.DataFrame(df.dtypes, columns=['Data Type']))\n",
    "        \n",
    "        with tab3:\n",
    "            missing = df.isnull().sum()\n",
    "            if missing.sum() > 0:\n",
    "                st.warning(f\"‚ö†Ô∏è Missing values detected in {(missing > 0).sum()} columns:\")\n",
    "                missing_df = pd.DataFrame({\n",
    "                    'Column': missing[missing > 0].index,\n",
    "                    'Missing Count': missing[missing > 0].values,\n",
    "                    'Missing %': (missing[missing > 0].values / len(df) * 100).round(2)\n",
    "                })\n",
    "                st.dataframe(missing_df)\n",
    "            else:\n",
    "                st.success(\"‚úÖ No missing values detected\")\n",
    "            \n",
    "            # Check for potential date columns\n",
    "            potential_dates = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "            if potential_dates:\n",
    "                st.info(f\"üóìÔ∏è Potential date columns detected: {', '.join(potential_dates)}\")\n",
    "        \n",
    "    else:\n",
    "        st.info(\"üëÜ Please upload a CSV file to begin\")\n",
    "\n",
    "# Step 2: Configure Analysis\n",
    "elif step == \"2Ô∏è‚É£ Configure Analysis\":\n",
    "    st.header(\"Step 2: Configure Your Analysis\")\n",
    "    \n",
    "    if st.session_state.df is None:\n",
    "        st.warning(\"‚ö†Ô∏è Please upload data first (Step 1)\")\n",
    "    else:\n",
    "        df = st.session_state.df\n",
    "        \n",
    "        # Core Variables Section\n",
    "        st.subheader(\"üéØ Core Variables\")\n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        \n",
    "        with col1:\n",
    "            date_col = st.selectbox(\"üìÖ Date Column\", df.columns.tolist(), key=\"date_col\")\n",
    "        \n",
    "        with col2:\n",
    "            outcome_col = st.selectbox(\"üìä Outcome Variable (Y)\", \n",
    "                                      [col for col in df.columns if col != date_col], \n",
    "                                      key=\"outcome_col\")\n",
    "        \n",
    "        with col3:\n",
    "            treatment_col = st.selectbox(\"üíâ Treatment Variable (D)\", \n",
    "                                        [col for col in df.columns if col not in [date_col, outcome_col]], \n",
    "                                        key=\"treatment_col\")\n",
    "        \n",
    "        # Confounders Section\n",
    "        st.markdown(\"---\")\n",
    "        st.subheader(\"üîß Confounding Variables\")\n",
    "        \n",
    "        available_cols = [col for col in df.columns if col not in [date_col, outcome_col, treatment_col]]\n",
    "        confounder_cols = st.multiselect(\n",
    "            \"Select confounding variables (X) - variables that affect both treatment and outcome\",\n",
    "            available_cols,\n",
    "            help=\"These are the observed covariates that may confound the causal relationship\",\n",
    "            key=\"confounder_cols\"\n",
    "        )\n",
    "        \n",
    "        # Treatment Transformation Section\n",
    "        st.markdown(\"---\")\n",
    "        st.subheader(\"üîÑ Treatment Transformation (Optional)\")\n",
    "        \n",
    "        col1, col2 = st.columns([1, 2])\n",
    "        \n",
    "        with col1:\n",
    "            treatment_transform = st.selectbox(\n",
    "                \"Transform treatment variable?\",\n",
    "                [\"None\", \"Adstock (Geometric Decay)\", \"Simple Lags\"],\n",
    "                help=\"Apply transformations to capture delayed/cumulative effects\"\n",
    "            )\n",
    "        \n",
    "        with col2:\n",
    "            if treatment_transform == \"Adstock (Geometric Decay)\":\n",
    "                st.info(\"**Adstock** models carryover effects (common in marketing/advertising)\")\n",
    "                col2a, col2b = st.columns(2)\n",
    "                with col2a:\n",
    "                    alpha = st.slider(\"Decay Rate (Œ±)\", 0.0, 1.0, 0.9, 0.05, \n",
    "                                     help=\"How quickly effects decay (higher = slower decay)\")\n",
    "                with col2b:\n",
    "                    lags = st.slider(\"Number of Lags\", 1, 30, 7,\n",
    "                                    help=\"How many periods to look back\")\n",
    "            elif treatment_transform == \"Simple Lags\":\n",
    "                st.info(\"**Simple Lags** use past values as features\")\n",
    "                lag_periods = st.multiselect(\"Lag periods\", [1, 7, 14, 30, 60, 90], default=[1, 7])\n",
    "        \n",
    "        # Feature Engineering Section\n",
    "        st.markdown(\"---\")\n",
    "        st.subheader(\"üõ†Ô∏è Feature Engineering\")\n",
    "        \n",
    "        st.write(\"Select which temporal features to automatically generate:\")\n",
    "        \n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        \n",
    "        with col1:\n",
    "            include_cyclical = st.checkbox(\"üìà Cyclical Features\", value=True,\n",
    "                                          help=\"Sin/cos encodings for month, day of week, day of year\")\n",
    "        \n",
    "        with col2:\n",
    "            include_calendar = st.checkbox(\"üìÖ Calendar Features\", value=True,\n",
    "                                          help=\"Weekend, month start/end, quarter flags\")\n",
    "        \n",
    "        with col3:\n",
    "            include_trend = st.checkbox(\"üìâ Trend Features\", value=True,\n",
    "                                       help=\"Linear and quadratic time trends\")\n",
    "        \n",
    "        with col4:\n",
    "            include_retail = st.checkbox(\"üõçÔ∏è Retail Events\", value=False,\n",
    "                                        help=\"Holiday season, Black Friday, Prime Day, etc.\")\n",
    "        \n",
    "        # Model Configuration Section\n",
    "        st.markdown(\"---\")\n",
    "        st.subheader(\"‚öôÔ∏è Model Configuration\")\n",
    "        \n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        \n",
    "        with col1:\n",
    "            n_folds = st.slider(\"CV Folds\", 2, 10, 5, \n",
    "                               help=\"Number of time series cross-validation folds\")\n",
    "        \n",
    "        with col2:\n",
    "            time_budget = st.slider(\"FLAML Time Budget (s)\", 30, 600, 120, 30,\n",
    "                                   help=\"Time budget per model (outcome & treatment)\")\n",
    "        \n",
    "        with col3:\n",
    "            estimators = st.multiselect(\n",
    "                \"ML Estimators\",\n",
    "                [\"lgbm\", \"xgboost\", \"histgb\", \"xgb_limitdepth\", \"catboost\", \"rf\", \"extra_tree\"],\n",
    "                default=[\"lgbm\", \"xgboost\", \"histgb\"],\n",
    "                help=\"Machine learning algorithms to try\"\n",
    "            )\n",
    "        \n",
    "        # Advanced Options\n",
    "        with st.expander(\"üî¨ Advanced Options\"):\n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                score_type = st.selectbox(\n",
    "                    \"DML Score Type\",\n",
    "                    [\"partialling out\", \"IV-type\"],\n",
    "                    help=\"Orthogonalization approach for DML\"\n",
    "                )\n",
    "                \n",
    "                n_rep = st.slider(\"Number of Repetitions\", 1, 10, 1,\n",
    "                                 help=\"Repeated sample splitting for stability\")\n",
    "            \n",
    "            with col2:\n",
    "                dml_procedure = st.selectbox(\n",
    "                    \"DML Procedure\",\n",
    "                    [\"dml1\", \"dml2\"],\n",
    "                    index=1,\n",
    "                    help=\"DML1: no cross-fitting, DML2: with cross-fitting\"\n",
    "                )\n",
    "                \n",
    "                trim_threshold = st.slider(\"Propensity Score Trimming\", 0.0, 0.2, 0.01, 0.01,\n",
    "                                          help=\"Trim extreme propensity scores (0 = no trimming)\")\n",
    "        \n",
    "        # Save Configuration\n",
    "        st.markdown(\"---\")\n",
    "        if st.button(\"‚úÖ Confirm Configuration\", type=\"primary\", use_container_width=True):\n",
    "            \n",
    "            # Validate configuration\n",
    "            if not confounder_cols:\n",
    "                st.warning(\"‚ö†Ô∏è Consider adding confounding variables for more robust estimates\")\n",
    "            \n",
    "            config = {\n",
    "                'date_col': date_col,\n",
    "                'outcome_col': outcome_col,\n",
    "                'treatment_col': treatment_col,\n",
    "                'confounder_cols': confounder_cols,\n",
    "                'treatment_transform': treatment_transform,\n",
    "                'n_folds': n_folds,\n",
    "                'time_budget': time_budget,\n",
    "                'estimators': estimators,\n",
    "                'include_cyclical': include_cyclical,\n",
    "                'include_calendar': include_calendar,\n",
    "                'include_trend': include_trend,\n",
    "                'include_retail': include_retail,\n",
    "                'score_type': score_type,\n",
    "                'n_rep': n_rep,\n",
    "                'dml_procedure': dml_procedure,\n",
    "                'trim_threshold': trim_threshold\n",
    "            }\n",
    "            \n",
    "            # Add transformation-specific params\n",
    "            if treatment_transform == \"Adstock (Geometric Decay)\":\n",
    "                config['alpha'] = alpha\n",
    "                config['lags'] = lags\n",
    "            elif treatment_transform == \"Simple Lags\":\n",
    "                config['lag_periods'] = lag_periods\n",
    "            \n",
    "            st.session_state.variable_config = config\n",
    "            \n",
    "            # Show configuration summary\n",
    "            st.success(\"‚úÖ Configuration saved! Here's your setup:\")\n",
    "            \n",
    "            summary_col1, summary_col2 = st.columns(2)\n",
    "            \n",
    "            with summary_col1:\n",
    "                st.write(\"**Variables:**\")\n",
    "                st.write(f\"- Outcome: `{outcome_col}`\")\n",
    "                st.write(f\"- Treatment: `{treatment_col}`\")\n",
    "                st.write(f\"- Confounders: {len(confounder_cols)}\")\n",
    "            \n",
    "            with summary_col2:\n",
    "                st.write(\"**Configuration:**\")\n",
    "                st.write(f\"- Transform: {treatment_transform}\")\n",
    "                st.write(f\"- CV Folds: {n_folds}\")\n",
    "                st.write(f\"- Time Budget: {time_budget}s\")\n",
    "            \n",
    "            st.info(\"üëâ Proceed to Step 3 to run the analysis\")\n",
    "\n",
    "# Step 3: Run Analysis\n",
    "elif step == \"3Ô∏è‚É£ Run Analysis\":\n",
    "    st.header(\"Step 3: Run Causal Analysis\")\n",
    "    \n",
    "    if st.session_state.df is None:\n",
    "        st.warning(\"‚ö†Ô∏è Please upload data first (Step 1)\")\n",
    "    elif 'variable_config' not in st.session_state:\n",
    "        st.warning(\"‚ö†Ô∏è Please configure your analysis first (Step 2)\")\n",
    "    else:\n",
    "        config = st.session_state.variable_config\n",
    "        \n",
    "        # Configuration Summary\n",
    "        st.subheader(\"üìã Configuration Summary\")\n",
    "        \n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        \n",
    "        with col1:\n",
    "            st.metric(\"Outcome\", config['outcome_col'])\n",
    "            st.metric(\"Treatment\", config['treatment_col'])\n",
    "        \n",
    "        with col2:\n",
    "            st.metric(\"Confounders\", len(config['confounder_cols']))\n",
    "            st.metric(\"Transform\", config['treatment_transform'].split()[0])\n",
    "        \n",
    "        with col3:\n",
    "            st.metric(\"CV Folds\", config['n_folds'])\n",
    "            st.metric(\"Estimators\", len(config['estimators']))\n",
    "        \n",
    "        with col4:\n",
    "            st.metric(\"Time Budget\", f\"{config['time_budget']}s\")\n",
    "            st.metric(\"DML Score\", config['score_type'].split()[0])\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "        \n",
    "        if st.button(\"üöÄ Run Analysis\", type=\"primary\", use_container_width=True):\n",
    "            with st.spinner(\"Running analysis... This may take several minutes.\"):\n",
    "                try:\n",
    "                    # Progress tracking\n",
    "                    progress_bar = st.progress(0)\n",
    "                    status_text = st.empty()\n",
    "                    \n",
    "                    # Step 1: Load and preprocess\n",
    "                    status_text.text(\"Step 1/7: Loading and preprocessing data...\")\n",
    "                    progress_bar.progress(5)\n",
    "                    \n",
    "                    df = st.session_state.df.copy()\n",
    "                    \n",
    "                    # Handle missing values in treatment\n",
    "                    if df[config['treatment_col']].isnull().any():\n",
    "                        df[config['treatment_col']].fillna(method='ffill', inplace=True)\n",
    "                    \n",
    "                    # Step 2: Apply treatment transformation\n",
    "                    status_text.text(\"Step 2/7: Applying treatment transformation...\")\n",
    "                    progress_bar.progress(15)\n",
    "                    \n",
    "                    treatment_var_name = config['treatment_col']\n",
    "                    \n",
    "                    if config['treatment_transform'] == \"Adstock (Geometric Decay)\":\n",
    "                        df[\"transformed_treatment\"] = geometric_adstock(\n",
    "                            df[config['treatment_col']], \n",
    "                            alpha=config['alpha'], \n",
    "                            lags=config['lags']\n",
    "                        )\n",
    "                        treatment_var_name = \"transformed_treatment\"\n",
    "                    elif config['treatment_transform'] == \"Simple Lags\":\n",
    "                        df = apply_lag_features(df, config['treatment_col'], config['lag_periods'])\n",
    "                        # Keep original as primary, but lags will be in confounders\n",
    "                        treatment_var_name = config['treatment_col']\n",
    "                    else:\n",
    "                        df[\"transformed_treatment\"] = df[config['treatment_col']]\n",
    "                        treatment_var_name = \"transformed_treatment\"\n",
    "                    \n",
    "                    # Drop rows with missing outcome or treatment\n",
    "                    df.dropna(subset=[config['outcome_col'], treatment_var_name], inplace=True)\n",
    "                    df.reset_index(drop=True, inplace=True)\n",
    "                    \n",
    "                    # Step 3: Feature engineering\n",
    "                    status_text.text(\"Step 3/7: Engineering temporal features...\")\n",
    "                    progress_bar.progress(25)\n",
    "                    \n",
    "                    transformer = TemporalFeatures(\n",
    "                        date_col=config['date_col'],\n",
    "                        include_cyclical=config['include_cyclical'],\n",
    "                        include_calendar=config['include_calendar'],\n",
    "                        include_trend=config['include_trend'],\n",
    "                        include_retail_events=config['include_retail']\n",
    "                    )\n",
    "                    \n",
    "                    df_temp, temp_feats = transformer.fit_transform(df)\n",
    "                    \n",
    "                    # Combine all features\n",
    "                    all_x = config['confounder_cols'] + temp_feats\n",
    "                    \n",
    "                    # Build model dataframe\n",
    "                    df_model = pd.concat([\n",
    "                        df_temp[temp_feats],\n",
    "                        df[config['confounder_cols']],\n",
    "                        df[[config['outcome_col'], treatment_var_name]]\n",
    "                    ], axis=1).loc[:, lambda d: ~d.columns.duplicated()]\n",
    "                    \n",
    "                    # Drop any remaining NaNs\n",
    "                    df_model.dropna(inplace=True)\n",
    "                    \n",
    "                    st.info(f\"üìä Analysis dataset: {len(df_model)} observations, {len(all_x)} features\")\n",
    "                    \n",
    "                    # Step 4: Set up cross-validation\n",
    "                    status_text.text(\"Step 4/7: Setting up time series cross-validation...\")\n",
    "                    progress_bar.progress(35)\n",
    "                    \n",
    "                    my_splitter = TimeSeriesSplit(n_splits=config['n_folds'])\n",
    "                    \n",
    "                    X_all = df_model[all_x]\n",
    "                    y_outcome = df_model[config['outcome_col']]\n",
    "                    d_treatment = df_model[treatment_var_name]\n",
    "                    \n",
    "                    # Step 5: Fit outcome model\n",
    "                    status_text.text(\"Step 5/7: Fitting outcome model (ml_l)...\")\n",
    "                    progress_bar.progress(45)\n",
    "                    \n",
    "                    automl_l = AutoML()\n",
    "                    automl_l.fit(\n",
    "                        X_train=X_all, y_train=y_outcome,\n",
    "                        task=\"regression\", metric=\"rmse\", \n",
    "                        time_budget=config['time_budget'],\n",
    "                        split_type=my_splitter, \n",
    "                        estimator_list=config['estimators'],\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    \n",
    "                    # Step 6: Fit treatment model\n",
    "                    status_text.text(\"Step 6/7: Fitting treatment model (ml_m)...\")\n",
    "                    progress_bar.progress(60)\n",
    "                    \n",
    "                    automl_m = AutoML()\n",
    "                    automl_m.fit(\n",
    "                        X_train=X_all, y_train=d_treatment,\n",
    "                        task=\"regression\", metric=\"rmse\", \n",
    "                        time_budget=config['time_budget'],\n",
    "                        split_type=my_splitter, \n",
    "                        estimator_list=config['estimators'],\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    \n",
    "                    # Step 7: Generate CV predictions\n",
    "                    status_text.text(\"Step 7/7: Generating cross-validated predictions and computing causal effect...\")\n",
    "                    progress_bar.progress(75)\n",
    "                    \n",
    "                    cv_preds_l = np.full_like(y_outcome, fill_value=np.nan, dtype=float)\n",
    "                    cv_preds_m = np.full_like(d_treatment, fill_value=np.nan, dtype=float)\n",
    "                    \n",
    "                    best_estimator_l = automl_l.model.estimator\n",
    "                    best_estimator_m = automl_m.model.estimator\n",
    "                    \n",
    "                    for fold_idx, (train_idx, test_idx) in enumerate(my_splitter.split(X_all)):\n",
    "                        best_estimator_l.fit(X_all.iloc[train_idx], y_outcome.iloc[train_idx])\n",
    "                        cv_preds_l[test_idx] = best_estimator_l.predict(X_all.iloc[test_idx])\n",
    "                        best_estimator_m.fit(X_all.iloc[train_idx], d_treatment.iloc[train_idx])\n",
    "                        cv_preds_m[test_idx] = best_estimator_m.predict(X_all.iloc[test_idx])\n",
    "                    \n",
    "                    mask = ~np.isnan(cv_preds_l)\n",
    "                    \n",
    "                    cv_loss_l = root_mean_squared_error(y_outcome[mask], cv_preds_l[mask])\n",
    "                    cv_loss_m = root_mean_squared_error(d_treatment[mask], cv_preds_m[mask])\n",
    "                    \n",
    "                    # Prepare DoubleML\n",
    "                    y_for_doubleml = y_outcome.iloc[mask]\n",
    "                    d_for_doubleml = d_treatment.iloc[mask]\n",
    "                    \n",
    "                    df_for_doubleml = pd.concat([\n",
    "                        pd.Series(y_for_doubleml, name=config['outcome_col'], index=y_for_doubleml.index),\n",
    "                        pd.Series(d_for_doubleml, name=treatment_var_name, index=y_for_doubleml.index),\n",
    "                        pd.Series(np.zeros_like(y_for_doubleml), name=\"dummy_x\", index=y_for_doubleml.index),\n",
    "                    ], axis=1)\n",
    "                    \n",
    "                    dml_data = dml.DoubleMLData(\n",
    "                        df_for_doubleml, \n",
    "                        y_col=config['outcome_col'], \n",
    "                        d_cols=treatment_var_name, \n",
    "                        x_cols=[\"dummy_x\"]\n",
    "                    )\n",
    "                    \n",
    "                    # Fit DoubleML\n",
    "                    progress_bar.progress(90)\n",
    "                    \n",
    "                    ml_l_dummy = dml.utils.DMLDummyRegressor()\n",
    "                    ml_m_dummy = dml.utils.DMLDummyRegressor()\n",
    "                    \n",
    "                    pred_dict = {\n",
    "                        treatment_var_name: {\n",
    "                            \"ml_l\": cv_preds_l[mask].reshape(-1, 1),\n",
    "                            \"ml_m\": cv_preds_m[mask].reshape(-1, 1),\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    dml_plr = dml.DoubleMLPLR(\n",
    "                        dml_data,\n",
    "                        ml_l=ml_l_dummy,\n",
    "                        ml_m=ml_m_dummy,\n",
    "                        n_folds=config['n_folds'],\n",
    "                        n_rep=config['n_rep'],\n",
    "                        score=config['score_type']\n",
    "                    )\n",
    "                    \n",
    "                    dml_plr.fit(external_predictions=pred_dict)\n",
    "                    \n",
    "                    progress_bar.progress(100)\n",
    "                    status_text.text(\"‚úÖ Analysis complete!\")\n",
    "                    \n",
    "                    # Store results\n",
    "                    st.session_state.results = {\n",
    "                        'dml_plr': dml_plr,\n",
    "                        'cv_loss_l': cv_loss_l,\n",
    "                        'cv_loss_m': cv_loss_m,\n",
    "                        'n_obs': dml_data.n_obs,\n",
    "                        'mask_sum': mask.sum(),\n",
    "                        'total_obs': len(df_model),\n",
    "                        'confounder_cols': config['confounder_cols'],\n",
    "                        'treatment_var_name': treatment_var_name,\n",
    "                        'best_estimator_l': automl_l.best_estimator,\n",
    "                        'best_estimator_m': automl_m.best_estimator,\n",
    "                        'temporal_features': temp_feats\n",
    "                    }\n",
    "                    st.session_state.dml_plr = dml_plr\n",
    "                    \n",
    "                    time.sleep(0.5)\n",
    "                    progress_bar.empty()\n",
    "                    status_text.empty()\n",
    "                    \n",
    "                    st.success(\"‚úÖ Analysis completed successfully!\")\n",
    "                    \n",
    "                    # Display Results\n",
    "                    st.markdown(\"---\")\n",
    "                    st.header(\"üìä Causal Effect Estimates\")\n",
    "                    \n",
    "                    # Main metrics\n",
    "                    col1, col2, col3 = st.columns(3)\n",
    "                    \n",
    "                    with col1:\n",
    "                        st.metric(\"Causal Effect (Œ∏)\", f\"{dml_plr.coef[0]:.6f}\",\n",
    "                                 help=\"Average treatment effect on the outcome\")\n",
    "                        st.metric(\"Standard Error\", f\"{dml_plr.se[0]:.6f}\")\n",
    "                    \n",
    "                    with col2:\n",
    "                        ci_lower = dml_plr.confint().iloc[0, 0]\n",
    "                        ci_upper = dml_plr.confint().iloc[0, 1]\n",
    "                        st.metric(\"95% CI Lower\", f\"{ci_lower:.6f}\")\n",
    "                        st.metric(\"95% CI Upper\", f\"{ci_upper:.6f}\")\n",
    "                    \n",
    "                    with col3:\n",
    "                        p_val = dml_plr.pval[0]\n",
    "                        st.metric(\"P-value\", f\"{p_val:.6f}\")\n",
    "                        if p_val < 0.05:\n",
    "                            st.success(\"‚úÖ Statistically significant (p < 0.05)\")\n",
    "                        else:\n",
    "                            st.warning(\"‚ö†Ô∏è Not statistically significant (p ‚â• 0.05)\")\n",
    "                    \n",
    "                    # Interpretation\n",
    "                    st.subheader(\"üìù Interpretation\")\n",
    "                    effect = dml_plr.coef[0]\n",
    "                    if effect > 0:\n",
    "                        st.info(f\"üìà A one-unit increase in **{config['treatment_col']}** is associated with a **{effect:.4f}** unit increase in **{config['outcome_col']}** (on average, all else equal).\")\n",
    "                    else:\n",
    "                        st.info(f\"üìâ A one-unit increase in **{config['treatment_col']}** is associated with a **{abs(effect):.4f}** unit decrease in **{config['outcome_col']}** (on average, all else equal).\")\n",
    "                    \n",
    "                    # Summary table\n",
    "                    st.subheader(\"üìã Detailed Summary\")\n",
    "                    summary_df = dml_plr.summary\n",
    "                    st.dataframe(summary_df, use_container_width=True)\n",
    "                    \n",
    "                    # Model Performance\n",
    "                    st.markdown(\"---\")\n",
    "                    st.subheader(\"üéØ Nuisance Model Performance\")\n",
    "                    \n",
    "                    perf_col1, perf_col2, perf_col3 = st.columns(3)\n",
    "                    \n",
    "                    with perf_col1:\n",
    "                        st.metric(\"Outcome Model RMSE\", f\"{cv_loss_l:.4f}\",\n",
    "                                 help=\"Lower is better - measures prediction accuracy for outcome\")\n",
    "                    \n",
    "                    with perf_col2:\n",
    "                        st.metric(\"Treatment Model RMSE\", f\"{cv_loss_m:.4f}\",\n",
    "                                 help=\"Lower is better - measures prediction accuracy for treatment\")\n",
    "                    \n",
    "                    with perf_col3:\n",
    "                        st.metric(\"Best Outcome Estimator\", automl_l.best_estimator)\n",
    "                        st.metric(\"Best Treatment Estimator\", automl_m.best_estimator)\n",
    "                    \n",
    "                    # Data Processing Details\n",
    "                    with st.expander(\"üìà Data Processing Details\"):\n",
    "                        st.write(f\"**Original observations:** {len(df_model)}\")\n",
    "                        st.write(f\"**Valid CV predictions:** {mask.sum()} ({100*mask.sum()/len(df_model):.1f}%)\")\n",
    "                        st.write(f\"**Used for DoubleML:** {dml_data.n_obs}\")\n",
    "                        st.write(f\"**Time series folds:** {config['n_folds']}\")\n",
    "                        st.write(f\"**Total features used:** {len(all_x)}\")\n",
    "                        st.write(f\"**Temporal features added:** {len(temp_feats)}\")\n",
    "                        \n",
    "                        if config['treatment_transform'] != \"None\":\n",
    "                            st.write(f\"**Treatment transformation:** {config['treatment_transform']}\")\n",
    "                    \n",
    "                    # Feature Importance (if available)\n",
    "                    with st.expander(\"üîç Feature Information\"):\n",
    "                        st.write(\"**Confounding Variables:**\")\n",
    "                        st.write(\", \".join(config['confounder_cols']) if config['confounder_cols'] else \"None\")\n",
    "                        \n",
    "                        st.write(\"\\n**Temporal Features Added:**\")\n",
    "                        st.write(\", \".join(temp_feats))\n",
    "                    \n",
    "                    st.markdown(\"---\")\n",
    "                    st.info(\"üëâ Proceed to Step 4 for sensitivity analysis to test robustness to unobserved confounding\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    st.error(f\"‚ùå Error during analysis: {str(e)}\")\n",
    "                    st.exception(e)\n",
    "                    progress_bar.empty()\n",
    "                    status_text.empty()\n",
    "\n",
    "# Step 4: Sensitivity Analysis\n",
    "elif step == \"4Ô∏è‚É£ Sensitivity Analysis\":\n",
    "    st.header(\"Step 4: Sensitivity Analysis\")\n",
    "    \n",
    "    if st.session_state.results is None:\n",
    "        st.warning(\"‚ö†Ô∏è Please run the analysis first (Step 3)\")\n",
    "    else:\n",
    "        dml_plr = st.session_state.results['dml_plr']\n",
    "        \n",
    "        st.info(\"\"\"\n",
    "        üìä **Sensitivity Analysis** assesses how robust your causal estimates are to potential **unobserved confounding**.\n",
    "        \n",
    "        Even after controlling for observed confounders, there may be unmeasured variables that affect both treatment and outcome.\n",
    "        This analysis shows how strong such confounding would need to be to change your conclusions.\n",
    "        \"\"\")\n",
    "        \n",
    "        # Sensitivity Parameters\n",
    "        st.subheader(\"üéõÔ∏è Sensitivity Parameters\")\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            cf_y = st.number_input(\n",
    "                \"Partial R¬≤ with outcome (cf_y)\",\n",
    "                min_value=0.0,\n",
    "                max_value=1.0,\n",
    "                value=0.05,\n",
    "                step=0.01,\n",
    "                format=\"%.4f\",\n",
    "                help=\"Strength of association between unobserved confounder and outcome (0-1)\"\n",
    "            )\n",
    "            \n",
    "            st.caption(\"üí° Represents how much of the outcome variance the unobserved confounder explains\")\n",
    "        \n",
    "        with col2:\n",
    "            cf_d = st.number_input(\n",
    "                \"Partial R¬≤ with treatment (cf_d)\",\n",
    "                min_value=0.0,\n",
    "                max_value=1.0,\n",
    "                value=0.05,\n",
    "                step=0.01,\n",
    "                format=\"%.4f\",\n",
    "                help=\"Strength of association between unobserved confounder and treatment (0-1)\"\n",
    "            )\n",
    "            \n",
    "            st.caption(\"üí° Represents how much of the treatment variance the unobserved confounder explains\")\n",
    "        \n",
    "        col3, col4 = st.columns(2)\n",
    "        \n",
    "        with col3:\n",
    "            rho = st.slider(\n",
    "                \"Correlation (œÅ)\",\n",
    "                min_value=-1.0,\n",
    "                max_value=1.0,\n",
    "                value=1.0,\n",
    "                step=0.1,\n",
    "                help=\"Correlation between confounding effects on outcome and treatment\"\n",
    "            )\n",
    "        \n",
    "        with col4:\n",
    "            level = st.slider(\n",
    "                \"Confidence Level\",\n",
    "                min_value=0.80,\n",
    "                max_value=0.99,\n",
    "                value=0.95,\n",
    "                step=0.01,\n",
    "                format=\"%.2f\"\n",
    "            )\n",
    "        \n",
    "        # Preset scenarios\n",
    "        st.subheader(\"üìã Preset Scenarios\")\n",
    "        \n",
    "        scenario = st.selectbox(\n",
    "            \"Or choose a preset scenario:\",\n",
    "            [\"Custom\", \"Weak Confounding\", \"Moderate Confounding\", \"Strong Confounding\", \"Extreme Confounding\"]\n",
    "        )\n",
    "        \n",
    "        if scenario == \"Weak Confounding\":\n",
    "            cf_y, cf_d = 0.01, 0.01\n",
    "        elif scenario == \"Moderate Confounding\":\n",
    "            cf_y, cf_d = 0.05, 0.05\n",
    "        elif scenario == \"Strong Confounding\":\n",
    "            cf_y, cf_d = 0.10, 0.10\n",
    "        elif scenario == \"Extreme Confounding\":\n",
    "            cf_y, cf_d = 0.20, 0.20\n",
    "        \n",
    "        # Benchmarking\n",
    "        st.markdown(\"---\")\n",
    "        st.subheader(\"üìä Benchmarking Against Observed Variables\")\n",
    "        \n",
    "        st.write(\"\"\"\n",
    "        Compare the sensitivity parameters to observed confounders to understand what level of \n",
    "        unobserved confounding would be needed to overturn your results.\n",
    "        \"\"\")\n",
    "        \n",
    "        available_vars = st.session_state.results['confounder_cols']\n",
    "        \n",
    "        if available_vars:\n",
    "            benchmark_vars = st.multiselect(\n",
    "                \"Select variables to use as benchmarks:\",\n",
    "                available_vars,\n",
    "                default=available_vars[:min(3, len(available_vars))],\n",
    "                help=\"These variables will be used to calibrate the sensitivity analysis\"\n",
    "            )\n",
    "        else:\n",
    "            st.warning(\"‚ö†Ô∏è No confounding variables available for benchmarking\")\n",
    "            benchmark_vars = []\n",
    "        \n",
    "        # Run Analysis Button\n",
    "        st.markdown(\"---\")\n",
    "        \n",
    "        if st.button(\"üîç Run Sensitivity Analysis\", type=\"primary\", use_container_width=True):\n",
    "            with st.spinner(\"Running sensitivity analysis...\"):\n",
    "                try:\n",
    "                    # Run sensitivity analysis\n",
    "                    dml_plr.sensitivity_analysis(cf_y=cf_y, cf_d=cf_d, rho=rho, level=level)\n",
    "                    \n",
    "                    st.success(\"‚úÖ Sensitivity analysis complete!\")\n",
    "                    \n",
    "                    # Display Summary\n",
    "                    st.markdown(\"---\")\n",
    "                    st.subheader(\"üìä Sensitivity Summary\")\n",
    "                    \n",
    "                    st.dataframe(dml_plr.sensitivity_summary, use_container_width=True)\n",
    "                    \n",
    "                    # Interpretation\n",
    "                    st.subheader(\"üìù Interpretation\")\n",
    "                    \n",
    "                    theta_lower = dml_plr.sensitivity_summary['theta_lower'].iloc[0]\n",
    "                    theta_upper = dml_plr.sensitivity_summary['theta_upper'].iloc[0]\n",
    "                    ci_lower = dml_plr.sensitivity_summary['ci_lower'].iloc[0]\n",
    "                    ci_upper = dml_plr.sensitivity_summary['ci_upper'].iloc[0]\n",
    "                    \n",
    "                    st.write(f\"\"\"\n",
    "                    Given unobserved confounding with:\n",
    "                    - **cf_y = {cf_y:.4f}** (explains {cf_y*100:.2f}% of outcome variance)\n",
    "                    - **cf_d = {cf_d:.4f}** (explains {cf_d*100:.2f}% of treatment variance)\n",
    "                    - **œÅ = {rho:.2f}** (correlation between confounding effects)\n",
    "                    \n",
    "                    The causal effect estimate would be bounded between **{theta_lower:.4f}** and **{theta_upper:.4f}**.\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    # Check if zero is in bounds\n",
    "                    if theta_lower < 0 < theta_upper:\n",
    "                        st.warning(\"‚ö†Ô∏è **Zero is within the sensitivity bounds**, suggesting the effect could be overturned by unobserved confounding of this magnitude.\")\n",
    "                    else:\n",
    "                        st.success(\"‚úÖ **Zero is not within the sensitivity bounds**, suggesting the effect is robust to this level of unobserved confounding.\")\n",
    "                    \n",
    "                    # Visualization\n",
    "                    st.markdown(\"---\")\n",
    "                    st.subheader(\"üìà Sensitivity Plots\")\n",
    "                    \n",
    "                    tab1, tab2 = st.tabs([\"Effect Bounds (Œ∏)\", \"Confidence Interval Bounds\"])\n",
    "                    \n",
    "                    with tab1:\n",
    "                        st.write(\"Shows how the point estimate changes with different levels of confounding\")\n",
    "                        fig_theta = dml_plr.sensitivity_plot(value='theta')\n",
    "                        st.pyplot(fig_theta)\n",
    "                        \n",
    "                        st.caption(\"\"\"\n",
    "                        The plot shows the estimated causal effect across different values of confounding strength.\n",
    "                        The shaded region represents the sensitivity bounds.\n",
    "                        \"\"\")\n",
    "                    \n",
    "                    with tab2:\n",
    "                        st.write(f\"Shows how the {level*100:.0f}% confidence interval changes with different levels of confounding\")\n",
    "                        fig_ci = dml_plr.sensitivity_plot(value='ci', level=level)\n",
    "                        st.pyplot(fig_ci)\n",
    "                        \n",
    "                        st.caption(\"\"\"\n",
    "                        The plot shows the confidence interval bounds across different values of confounding strength.\n",
    "                        If the bounds cross zero, the effect becomes statistically insignificant.\n",
    "                        \"\"\")\n",
    "                    \n",
    "                    # Benchmarking Results\n",
    "                    if benchmark_vars:\n",
    "                        st.markdown(\"---\")\n",
    "                        st.subheader(\"üéØ Benchmarking Results\")\n",
    "                        \n",
    "                        st.write(\"\"\"\n",
    "                        These results compare the sensitivity parameters to observed confounders,\n",
    "                        helping you understand what \"strength\" of unobserved confounding would be needed.\n",
    "                        \"\"\")\n",
    "                        \n",
    "                        for var in benchmark_vars:\n",
    "                            with st.expander(f\"üìä Benchmark: **{var}**\"):\n",
    "                                try:\n",
    "                                    bench_result = dml_plr.sensitivity_benchmark(benchmarking_set=[var])\n",
    "                                    \n",
    "                                    if isinstance(bench_result, dict):\n",
    "                                        st.write(\"**Benchmark Parameters:**\")\n",
    "                                        for key, value in bench_result.items():\n",
    "                                            st.write(f\"- {key}: {value}\")\n",
    "                                    else:\n",
    "                                        st.write(bench_result)\n",
    "                                    \n",
    "                                    st.info(f\"\"\"\n",
    "                                    This shows the confounding strength of **{var}**. \n",
    "                                    If unobserved confounding is similar in strength to **{var}**, \n",
    "                                    these would be the sensitivity parameters.\n",
    "                                    \"\"\")\n",
    "                                    \n",
    "                                except Exception as e:\n",
    "                                    st.error(f\"Error benchmarking {var}: {str(e)}\")\n",
    "                    \n",
    "                    # Additional Insights\n",
    "                    st.markdown(\"---\")\n",
    "                    st.subheader(\"üí° Key Insights\")\n",
    "                    \n",
    "                    with st.expander(\"ü§î How to interpret these results\"):\n",
    "                        st.markdown(\"\"\"\n",
    "                        **Sensitivity Analysis Guidelines:**\n",
    "                        \n",
    "                        1. **Small cf_y and cf_d values (< 0.05)**: Your results are robust to weak unobserved confounding\n",
    "                        2. **Moderate values (0.05 - 0.15)**: Results require careful interpretation\n",
    "                        3. **Large values (> 0.15)**: Results are sensitive to unobserved confounding\n",
    "                        \n",
    "                        **Benchmarking helps you answer:**\n",
    "                        - \"How strong would an unobserved confounder need to be compared to my observed confounders?\"\n",
    "                        - \"Is it plausible that such a confounder exists in my context?\"\n",
    "                        \n",
    "                        **Best practices:**\n",
    "                        - Compare cf_y and cf_d to R¬≤ values from your observed confounders\n",
    "                        - Consider domain knowledge about potential unobserved confounders\n",
    "                        - Report sensitivity results alongside main estimates\n",
    "                        - If results are sensitive, consider collecting additional covariates\n",
    "                        \"\"\")\n",
    "                    \n",
    "                    with st.expander(\"üìö Learn More\"):\n",
    "                        st.markdown(\"\"\"\n",
    "                        **Resources:**\n",
    "                        - [DoubleML Documentation](https://docs.doubleml.org/)\n",
    "                        - Chernozhukov et al. (2018): \"Double/debiased machine learning for treatment and structural parameters\"\n",
    "                        - Cinelli & Hazlett (2020): \"Making sense of sensitivity: Extending omitted variable bias\"\n",
    "                        \n",
    "                        **Citation:**\n",
    "                        If you use this tool in research, please cite the DoubleML package and relevant methodology papers.\n",
    "                        \"\"\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    st.error(f\"‚ùå Error during sensitivity analysis: {str(e)}\")\n",
    "                    st.exception(e)\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    <div style='text-align: center; color: #666;'>\n",
    "        <p><strong>Generic DoubleML Causal Analysis Tool</strong></p>\n",
    "        <p>Built with Streamlit | Powered by DoubleML & FLAML</p>\n",
    "        <p style='font-size: 0.9em;'>Support for Marketing, Healthcare, Policy, Finance & More</p>\n",
    "    </div>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
